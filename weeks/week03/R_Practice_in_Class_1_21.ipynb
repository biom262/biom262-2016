{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hot<-read.table(\"hot.txt\",header=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>animal</th><th scope=col>pair</th><th scope=col>hotsix</th><th scope=col>lukewarm</th><th scope=col>tepid</th><th scope=col>mild1</th><th scope=col>athermal</th><th scope=col>bathwater</th><th scope=col>coldshower</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>3011</td><td>1</td><td>-</td><td>0.4202</td><td>1.075</td><td>0.3974</td><td>0.9355</td><td>0.4695</td><td>0.1688</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>3022</td><td>2</td><td>-</td><td>0.4718</td><td>1.0288</td><td>1.2747</td><td>0.6859</td><td>0.5616</td><td>0.4862</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>3026</td><td>3</td><td>-</td><td>0.5351</td><td>0.9428</td><td>1.0208</td><td>0.5423</td><td>0.1972</td><td>0.2022</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>3045</td><td>4</td><td>-</td><td>0.4955</td><td>0.7869</td><td>0.9073</td><td>0.6276</td><td>0.1854</td><td>0.2918</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>3052</td><td>5</td><td>-</td><td>0.4299</td><td>0.9417</td><td>1.3223</td><td>0.8431</td><td>0.3982</td><td>0.3084</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>3059</td><td>6</td><td>-</td><td>0.5609</td><td>1.232</td><td>0.4473</td><td>1.5524</td><td>0.2859</td><td>0.1451</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>3066</td><td>7</td><td>-</td><td>1.4554</td><td>1.0419</td><td>0.6562</td><td>1.4554</td><td>0.4248</td><td>0.1613</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>3080</td><td>8</td><td>-</td><td>0.8959</td><td>0.7659</td><td>0.4807</td><td>1.1754</td><td>0.2734</td><td>0.1762</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>3082</td><td>9</td><td>-</td><td>0.3992</td><td>0.7921</td><td>1.4045</td><td>0.9383</td><td>0.472</td><td>0.3975</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>3110</td><td>10</td><td>-</td><td>0.3526</td><td>0.7998</td><td>1.2606</td><td>0.8413</td><td>0.4511</td><td>0.3654</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>3118</td><td>11</td><td>-</td><td>0.4693</td><td>1.0066</td><td>1.1801</td><td>1.0622</td><td>0.4872</td><td>0.2672</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>3012</td><td>1</td><td>+</td><td>0.444</td><td>1.4432</td><td>0.4406</td><td>1.6114</td><td>0.4515</td><td>0.2432</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>3020</td><td>2</td><td>+</td><td>0.6421</td><td>1.0284</td><td>1.3161</td><td>0.8921</td><td>0.6262</td><td>0.54</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>3023</td><td>3</td><td>+</td><td>0.476</td><td>0.7981</td><td>1</td><td>0.5934</td><td>0.231</td><td>0.3009</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>3040</td><td>4</td><td>+</td><td>0.6719</td><td>0.8398</td><td>1.274</td><td>0.7259</td><td>0.2996</td><td>0.3784</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>3049</td><td>5</td><td>+</td><td>0.5112</td><td>0.9713</td><td>1.3513</td><td>0.7823</td><td>0.3387</td><td>0.403</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>3056</td><td>6</td><td>+</td><td>0.6103</td><td>1.3715</td><td>0.51</td><td>1.7307</td><td>0.3617</td><td>0.1874</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>3079</td><td>7</td><td>+</td><td>1.6036</td><td>1.1495</td><td>0.7285</td><td>1.3375</td><td>0.4124</td><td>0.2952</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>3081</td><td>8</td><td>+</td><td>1.1222</td><td>0.8942</td><td>0.513</td><td>1.2949</td><td>0.4346</td><td>0.2583</td></tr>\n",
       "\t<tr><th scope=row>20</th><td>3085</td><td>9</td><td>+</td><td>0.4161</td><td>0.9395</td><td>1.1992</td><td>0.9451</td><td>0.6665</td><td>0.4637</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>3107</td><td>10</td><td>+</td><td>0.5402</td><td>0.9351</td><td>1.3654</td><td>1.0016</td><td>0.5347</td><td>0.5092</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>3121</td><td>11</td><td>+</td><td>0.4242</td><td>1.2581</td><td>1.311</td><td>1.1429</td><td>0.5469</td><td>0.4098</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllll}\n",
       "  & animal & pair & hotsix & lukewarm & tepid & mild1 & athermal & bathwater & coldshower\\\\\n",
       "\\hline\n",
       "\t1 & 3011 & 1 & - & 0.4202 & 1.075 & 0.3974 & 0.9355 & 0.4695 & 0.1688\\\\\n",
       "\t2 & 3022 & 2 & - & 0.4718 & 1.0288 & 1.2747 & 0.6859 & 0.5616 & 0.4862\\\\\n",
       "\t3 & 3026 & 3 & - & 0.5351 & 0.9428 & 1.0208 & 0.5423 & 0.1972 & 0.2022\\\\\n",
       "\t4 & 3045 & 4 & - & 0.4955 & 0.7869 & 0.9073 & 0.6276 & 0.1854 & 0.2918\\\\\n",
       "\t5 & 3052 & 5 & - & 0.4299 & 0.9417 & 1.3223 & 0.8431 & 0.3982 & 0.3084\\\\\n",
       "\t6 & 3059 & 6 & - & 0.5609 & 1.232 & 0.4473 & 1.5524 & 0.2859 & 0.1451\\\\\n",
       "\t7 & 3066 & 7 & - & 1.4554 & 1.0419 & 0.6562 & 1.4554 & 0.4248 & 0.1613\\\\\n",
       "\t8 & 3080 & 8 & - & 0.8959 & 0.7659 & 0.4807 & 1.1754 & 0.2734 & 0.1762\\\\\n",
       "\t9 & 3082 & 9 & - & 0.3992 & 0.7921 & 1.4045 & 0.9383 & 0.472 & 0.3975\\\\\n",
       "\t10 & 3110 & 10 & - & 0.3526 & 0.7998 & 1.2606 & 0.8413 & 0.4511 & 0.3654\\\\\n",
       "\t11 & 3118 & 11 & - & 0.4693 & 1.0066 & 1.1801 & 1.0622 & 0.4872 & 0.2672\\\\\n",
       "\t12 & 3012 & 1 & + & 0.444 & 1.4432 & 0.4406 & 1.6114 & 0.4515 & 0.2432\\\\\n",
       "\t13 & 3020 & 2 & + & 0.6421 & 1.0284 & 1.3161 & 0.8921 & 0.6262 & 0.54\\\\\n",
       "\t14 & 3023 & 3 & + & 0.476 & 0.7981 & 1 & 0.5934 & 0.231 & 0.3009\\\\\n",
       "\t15 & 3040 & 4 & + & 0.6719 & 0.8398 & 1.274 & 0.7259 & 0.2996 & 0.3784\\\\\n",
       "\t16 & 3049 & 5 & + & 0.5112 & 0.9713 & 1.3513 & 0.7823 & 0.3387 & 0.403\\\\\n",
       "\t17 & 3056 & 6 & + & 0.6103 & 1.3715 & 0.51 & 1.7307 & 0.3617 & 0.1874\\\\\n",
       "\t18 & 3079 & 7 & + & 1.6036 & 1.1495 & 0.7285 & 1.3375 & 0.4124 & 0.2952\\\\\n",
       "\t19 & 3081 & 8 & + & 1.1222 & 0.8942 & 0.513 & 1.2949 & 0.4346 & 0.2583\\\\\n",
       "\t20 & 3085 & 9 & + & 0.4161 & 0.9395 & 1.1992 & 0.9451 & 0.6665 & 0.4637\\\\\n",
       "\t21 & 3107 & 10 & + & 0.5402 & 0.9351 & 1.3654 & 1.0016 & 0.5347 & 0.5092\\\\\n",
       "\t22 & 3121 & 11 & + & 0.4242 & 1.2581 & 1.311 & 1.1429 & 0.5469 & 0.4098\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "   animal pair hotsix lukewarm  tepid  mild1 athermal bathwater coldshower\n",
       "1    3011    1      -   0.4202 1.0750 0.3974   0.9355    0.4695     0.1688\n",
       "2    3022    2      -   0.4718 1.0288 1.2747   0.6859    0.5616     0.4862\n",
       "3    3026    3      -   0.5351 0.9428 1.0208   0.5423    0.1972     0.2022\n",
       "4    3045    4      -   0.4955 0.7869 0.9073   0.6276    0.1854     0.2918\n",
       "5    3052    5      -   0.4299 0.9417 1.3223   0.8431    0.3982     0.3084\n",
       "6    3059    6      -   0.5609 1.2320 0.4473   1.5524    0.2859     0.1451\n",
       "7    3066    7      -   1.4554 1.0419 0.6562   1.4554    0.4248     0.1613\n",
       "8    3080    8      -   0.8959 0.7659 0.4807   1.1754    0.2734     0.1762\n",
       "9    3082    9      -   0.3992 0.7921 1.4045   0.9383    0.4720     0.3975\n",
       "10   3110   10      -   0.3526 0.7998 1.2606   0.8413    0.4511     0.3654\n",
       "11   3118   11      -   0.4693 1.0066 1.1801   1.0622    0.4872     0.2672\n",
       "12   3012    1      +   0.4440 1.4432 0.4406   1.6114    0.4515     0.2432\n",
       "13   3020    2      +   0.6421 1.0284 1.3161   0.8921    0.6262     0.5400\n",
       "14   3023    3      +   0.4760 0.7981 1.0000   0.5934    0.2310     0.3009\n",
       "15   3040    4      +   0.6719 0.8398 1.2740   0.7259    0.2996     0.3784\n",
       "16   3049    5      +   0.5112 0.9713 1.3513   0.7823    0.3387     0.4030\n",
       "17   3056    6      +   0.6103 1.3715 0.5100   1.7307    0.3617     0.1874\n",
       "18   3079    7      +   1.6036 1.1495 0.7285   1.3375    0.4124     0.2952\n",
       "19   3081    8      +   1.1222 0.8942 0.5130   1.2949    0.4346     0.2583\n",
       "20   3085    9      +   0.4161 0.9395 1.1992   0.9451    0.6665     0.4637\n",
       "21   3107   10      +   0.5402 0.9351 1.3654   1.0016    0.5347     0.5092\n",
       "22   3121   11      +   0.4242 1.2581 1.3110   1.1429    0.5469     0.4098"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following objects are masked from hot (pos = 3):\n",
      "\n",
      "    animal, athermal, bathwater, coldshower, hotsix, lukewarm, mild1,\n",
      "    pair, tepid\n",
      "\n",
      "The following objects are masked from hot (pos = 4):\n",
      "\n",
      "    animal, athermal, bathwater, coldshower, hotsix, lukewarm, mild1,\n",
      "    pair, tepid\n",
      "\n",
      "The following objects are masked from hot (pos = 5):\n",
      "\n",
      "    animal, athermal, bathwater, coldshower, hotsix, lukewarm, mild1,\n",
      "    pair, tepid\n",
      "\n",
      "The following objects are masked from hot (pos = 6):\n",
      "\n",
      "    animal, athermal, bathwater, coldshower, hotsix, lukewarm, mild1,\n",
      "    pair, tepid\n",
      "\n",
      "The following objects are masked from hot (pos = 7):\n",
      "\n",
      "    animal, athermal, bathwater, coldshower, hotsix, lukewarm, mild1,\n",
      "    pair, tepid\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  lukewarm[hotsix == \"+\"] - lukewarm[hotsix == \"-\"]\n",
       "W = 0.93038, p-value = 0.4147\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attach(hot)\n",
    "shapiro.test(lukewarm[hotsix=='+']-lukewarm[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  tepid[hotsix == \"+\"] - tepid[hotsix == \"-\"]\n",
       "W = 0.95915, p-value = 0.7611\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro.test(tepid[hotsix=='+']-tepid[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  mild1[hotsix == \"+\"] - mild1[hotsix == \"-\"]\n",
       "W = 0.86416, p-value = 0.06518\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro.test(mild1[hotsix=='+']-mild1[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  athermal[hotsix == \"+\"] - athermal[hotsix == \"-\"]\n",
       "W = 0.81042, p-value = 0.0129\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro.test(athermal[hotsix=='+']-athermal[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  bathwater[hotsix == \"+\"] - bathwater[hotsix == \"-\"]\n",
       "W = 0.97651, p-value = 0.9437\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro.test(bathwater[hotsix=='+']-bathwater[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tShapiro-Wilk normality test\n",
       "\n",
       "data:  coldshower[hotsix == \"+\"] - coldshower[hotsix == \"-\"]\n",
       "W = 0.92912, p-value = 0.402\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapiro.test(coldshower[hotsix=='+']-coldshower[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:7: unexpected symbol\n1: These tests\n          ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:7: unexpected symbol\n1: These tests\n          ^\n"
     ]
    }
   ],
   "source": [
    "These tests are used to ask are the data normally distributed? If you want to do a nonparimetric test you need to know\n",
    "distribution and\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  tepid[hotsix == \"+\"] and tepid[hotsix == \"-\"]\n",
       "F = 2.1475, num df = 10, denom df = 10, p-value = 0.2439\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.5777957 7.9819832\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "          2.147546 \n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(tepid[hotsix=='+'],tepid[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  lukewarm[hotsix == \"+\"] and lukewarm[hotsix == \"-\"]\n",
       "F = 1.2951, num df = 10, denom df = 10, p-value = 0.6904\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.3484517 4.8137003\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "          1.295122 \n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(lukewarm[hotsix=='+'],lukewarm[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  mild1[hotsix == \"+\"] and mild1[hotsix == \"-\"]\n",
       "F = 0.97389, num df = 10, denom df = 10, p-value = 0.9675\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.262025 3.619756\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "         0.9738925 \n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(mild1[hotsix=='+'],mild1[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  athermal[hotsix == \"+\"] and athermal[hotsix == \"-\"]\n",
       "F = 1.2772, num df = 10, denom df = 10, p-value = 0.7063\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.3436193 4.7469432\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "          1.277161 \n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(athermal[hotsix=='+'],athermal[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  bathwater[hotsix == \"+\"] and bathwater[hotsix == \"-\"]\n",
       "F = 1.1685, num df = 10, denom df = 10, p-value = 0.8103\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.314394 4.343209\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "          1.168537 \n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(bathwater[hotsix=='+'],bathwater[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tF test to compare two variances\n",
       "\n",
       "data:  coldshower[hotsix == \"+\"] and coldshower[hotsix == \"-\"]\n",
       "F = 1.0509, num df = 10, denom df = 10, p-value = 0.939\n",
       "alternative hypothesis: true ratio of variances is not equal to 1\n",
       "95 percent confidence interval:\n",
       " 0.2827324 3.9058186\n",
       "sample estimates:\n",
       "ratio of variances \n",
       "          1.050858 \n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.test(coldshower[hotsix=='+'],coldshower[hotsix=='-'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In the cell below we are calling lukewarm as a fxn of, this is what the tilde means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  lukewarm by hotsix\n",
       "t = -2.983, df = 10, p-value = 0.006868\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "        -Inf -0.03481709\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "            -0.08872727 \n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(lukewarm~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  tepid by hotsix\n",
       "t = -2.7562, df = 10, p-value = 0.01013\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "        -Inf -0.03782581\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "             -0.1104727 \n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(tepid~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  mild1 by hotsix\n",
       "t = -1.4748, df = 10, p-value = 0.08552\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "       -Inf 0.01367922\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "            -0.05974545 \n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(mild1~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  athermal by hotsix\n",
       "t = -2.0338, df = 10, p-value = 0.03468\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "        -Inf -0.01383605\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "             -0.1271273 \n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(athermal~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  bathwater by hotsix\n",
       "t = -2.7552, df = 10, p-value = 0.01015\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "        -Inf -0.02169678\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "            -0.06340909 \n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(bathwater~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\tPaired t-test\n",
       "\n",
       "data:  coldshower by hotsix\n",
       "t = -8.8405, df = 10, p-value = 2.429e-06\n",
       "alternative hypothesis: true difference in means is less than 0\n",
       "95 percent confidence interval:\n",
       "        -Inf -0.07364416\n",
       "sample estimates:\n",
       "mean of the differences \n",
       "            -0.09263636 \n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.test(coldshower~hotsix,paired=T,alternative=\"less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pt<-c(0.006868,0.01013,0.08552,0.03468,0.01015,2.429e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for p.adjust {stats}\"><tr><td>p.adjust {stats}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Adjust P-values for Multiple Comparisons</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Given a set of p-values, returns p-values adjusted using\n",
       "one of several methods.</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "p.adjust(p, method = p.adjust.methods, n = length(p))\n",
       "\n",
       "p.adjust.methods\n",
       "# c(\"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\",\n",
       "#   \"fdr\", \"none\")\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>p</code></td>\n",
       "<td>\n",
       "<p>numeric vector of p-values (possibly with <code>NA</code>s).\n",
       "Any other <span style=\"font-family: Courier New, Courier; color: #666666;\"><b>R</b></span> is coerced by <code>as.numeric</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>method</code></td>\n",
       "<td>\n",
       "<p>correction method.  Can be abbreviated.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>n</code></td>\n",
       "<td>\n",
       "<p>number of comparisons, must be at least <code>length(p)</code>;\n",
       "only set this (to non-default) when you know what you are doing!</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>The adjustment methods include the Bonferroni correction\n",
       "(<code>\"bonferroni\"</code>) in which the p-values are multiplied by the\n",
       "number of comparisons.  Less conservative corrections are also\n",
       "included by Holm (1979) (<code>\"holm\"</code>), Hochberg (1988)\n",
       "(<code>\"hochberg\"</code>), Hommel (1988) (<code>\"hommel\"</code>), Benjamini &amp;\n",
       "Hochberg (1995) (<code>\"BH\"</code> or its alias <code>\"fdr\"</code>), and Benjamini &amp;\n",
       "Yekutieli (2001) (<code>\"BY\"</code>), respectively.\n",
       "A pass-through option (<code>\"none\"</code>) is also included.\n",
       "The set of methods are contained in the <code>p.adjust.methods</code> vector\n",
       "for the benefit of methods that need to have the method as an option\n",
       "and pass it on to <code>p.adjust</code>.\n",
       "</p>\n",
       "<p>The first four methods are designed to give strong control of the\n",
       "family-wise error rate.  There seems no reason to use the unmodified\n",
       "Bonferroni correction because it is dominated by Holm's method, which\n",
       "is also valid under arbitrary assumptions.\n",
       "</p>\n",
       "<p>Hochberg's and Hommel's methods are valid when the hypothesis tests\n",
       "are independent or when they are non-negatively associated (Sarkar,\n",
       "1998; Sarkar and Chang, 1997).  Hommel's method is more powerful than\n",
       "Hochberg's, but the difference is usually small and the Hochberg\n",
       "p-values are faster to compute.\n",
       "</p>\n",
       "<p>The <code>\"BH\"</code> (aka <code>\"fdr\"</code>) and <code>\"BY\"</code> method of\n",
       "Benjamini, Hochberg, and Yekutieli control the false discovery rate,\n",
       "the expected proportion of false discoveries amongst the rejected\n",
       "hypotheses.  The false discovery rate is a less stringent condition\n",
       "than the family-wise error rate, so these methods are more powerful\n",
       "than the others.\n",
       "</p>\n",
       "<p>Note that you can set <code>n</code> larger than <code>length(p)</code> which\n",
       "means the unobserved p-values are assumed to be greater than all the\n",
       "observed p for <code>\"bonferroni\"</code> and <code>\"holm\"</code> methods and equal\n",
       "to 1 for the other methods.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>A numeric vector of corrected p-values (of the same length as\n",
       "<code>p</code>, with names copied from <code>p</code>).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Benjamini, Y., and Hochberg, Y. (1995).\n",
       "Controlling the false discovery rate: a practical and powerful\n",
       "approach to multiple testing.\n",
       "<em>Journal of the Royal Statistical Society Series B</em> <b>57</b>,\n",
       "289&ndash;300.\n",
       "</p>\n",
       "<p>Benjamini, Y., and Yekutieli, D. (2001).\n",
       "The control of the false discovery rate in multiple testing under\n",
       "dependency.\n",
       "<em>Annals of Statistics</em> <b>29</b>, 1165&ndash;1188.\n",
       "</p>\n",
       "<p>Holm, S. (1979).\n",
       "A simple sequentially rejective multiple test procedure.\n",
       "<em>Scandinavian Journal of Statistics</em> <b>6</b>, 65&ndash;70.\n",
       "</p>\n",
       "<p>Hommel, G. (1988).\n",
       "A stagewise rejective multiple test procedure based on a modified\n",
       "Bonferroni test.\n",
       "<em>Biometrika</em> <b>75</b>, 383&ndash;386.\n",
       "</p>\n",
       "<p>Hochberg, Y. (1988).\n",
       "A sharper Bonferroni procedure for multiple tests of significance.\n",
       "<em>Biometrika</em> <b>75</b>, 800&ndash;803.\n",
       "</p>\n",
       "<p>Shaffer, J. P. (1995).\n",
       "Multiple hypothesis testing.\n",
       "<em>Annual Review of Psychology</em> <b>46</b>, 561&ndash;576.\n",
       "(An excellent review of the area.)\n",
       "</p>\n",
       "<p>Sarkar, S. (1998).\n",
       "Some probability inequalities for ordered MTP2 random variables: a\n",
       "proof of Simes conjecture.\n",
       "<em>Annals of Statistics</em> <b>26</b>, 494&ndash;504.\n",
       "</p>\n",
       "<p>Sarkar, S., and Chang, C. K. (1997).\n",
       "Simes' method for multiple hypothesis testing with positively\n",
       "dependent test statistics.\n",
       "<em>Journal of the American Statistical Association</em> <b>92</b>,\n",
       "1601&ndash;1608.\n",
       "</p>\n",
       "<p>Wright, S. P. (1992).\n",
       "Adjusted P-values for simultaneous inference.\n",
       "<em>Biometrics</em> <b>48</b>, 1005&ndash;1013.\n",
       "(Explains the adjusted P-value approach.)\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>pairwise.*</code> functions such as <code>pairwise.t.test</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "require(graphics)\n",
       "\n",
       "set.seed(123)\n",
       "x &lt;- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\n",
       "p &lt;- 2*pnorm(sort(-abs(x)))\n",
       "\n",
       "round(p, 3)\n",
       "round(p.adjust(p), 3)\n",
       "round(p.adjust(p, \"BH\"), 3)\n",
       "\n",
       "## or all of them at once (dropping the \"fdr\" alias):\n",
       "p.adjust.M &lt;- p.adjust.methods[p.adjust.methods != \"fdr\"]\n",
       "p.adj    &lt;- sapply(p.adjust.M, function(meth) p.adjust(p, meth))\n",
       "p.adj.60 &lt;- sapply(p.adjust.M, function(meth) p.adjust(p, meth, n = 60))\n",
       "stopifnot(identical(p.adj[,\"none\"], p), p.adj &lt;= p.adj.60)\n",
       "round(p.adj, 3)\n",
       "## or a bit nicer:\n",
       "noquote(apply(p.adj, 2, format.pval, digits = 3))\n",
       "\n",
       "\n",
       "## and a graphic:\n",
       "matplot(p, p.adj, ylab=\"p.adjust(p, meth)\", type = \"l\", asp = 1, lty = 1:6,\n",
       "        main = \"P-value adjustments\")\n",
       "legend(0.7, 0.6, p.adjust.M, col = 1:6, lty = 1:6)\n",
       "\n",
       "## Can work with NA's:\n",
       "pN &lt;- p; iN &lt;- c(46, 47); pN[iN] &lt;- NA\n",
       "pN.a &lt;- sapply(p.adjust.M, function(meth) p.adjust(pN, meth))\n",
       "## The smallest 20 P-values all affected by the NA's :\n",
       "round((pN.a / p.adj)[1:20, ] , 4)\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>stats</em> version 3.2.2 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{p.adjust}{Adjust P-values for Multiple Comparisons}{p.adjust}\n",
       "\\methaliasA{p.adjust.methods}{p.adjust}{p.adjust.methods}\n",
       "\\keyword{htest}{p.adjust}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Given a set of p-values, returns p-values adjusted using\n",
       "one of several methods.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "p.adjust(p, method = p.adjust.methods, n = length(p))\n",
       "\n",
       "p.adjust.methods\n",
       "# c(\"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\",\n",
       "#   \"fdr\", \"none\")\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{p}] numeric vector of p-values (possibly with \\code{\\LinkA{NA}{NA}}s).\n",
       "Any other \\R{} is coerced by \\code{\\LinkA{as.numeric}{as.numeric}}.\n",
       "\\item[\\code{method}] correction method.  Can be abbreviated.\n",
       "\\item[\\code{n}] number of comparisons, must be at least \\code{length(p)};\n",
       "only set this (to non-default) when you know what you are doing!\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "The adjustment methods include the Bonferroni correction\n",
       "(\\code{\"bonferroni\"}) in which the p-values are multiplied by the\n",
       "number of comparisons.  Less conservative corrections are also\n",
       "included by Holm (1979) (\\code{\"holm\"}), Hochberg (1988)\n",
       "(\\code{\"hochberg\"}), Hommel (1988) (\\code{\"hommel\"}), Benjamini \\&\n",
       "Hochberg (1995) (\\code{\"BH\"} or its alias \\code{\"fdr\"}), and Benjamini \\&\n",
       "Yekutieli (2001) (\\code{\"BY\"}), respectively.\n",
       "A pass-through option (\\code{\"none\"}) is also included.\n",
       "The set of methods are contained in the \\code{p.adjust.methods} vector\n",
       "for the benefit of methods that need to have the method as an option\n",
       "and pass it on to \\code{p.adjust}.\n",
       "\n",
       "The first four methods are designed to give strong control of the\n",
       "family-wise error rate.  There seems no reason to use the unmodified\n",
       "Bonferroni correction because it is dominated by Holm's method, which\n",
       "is also valid under arbitrary assumptions.\n",
       "\n",
       "Hochberg's and Hommel's methods are valid when the hypothesis tests\n",
       "are independent or when they are non-negatively associated (Sarkar,\n",
       "1998; Sarkar and Chang, 1997).  Hommel's method is more powerful than\n",
       "Hochberg's, but the difference is usually small and the Hochberg\n",
       "p-values are faster to compute.\n",
       "\n",
       "The \\code{\"BH\"} (aka \\code{\"fdr\"}) and \\code{\"BY\"} method of\n",
       "Benjamini, Hochberg, and Yekutieli control the false discovery rate,\n",
       "the expected proportion of false discoveries amongst the rejected\n",
       "hypotheses.  The false discovery rate is a less stringent condition\n",
       "than the family-wise error rate, so these methods are more powerful\n",
       "than the others.\n",
       "\n",
       "Note that you can set \\code{n} larger than \\code{length(p)} which\n",
       "means the unobserved p-values are assumed to be greater than all the\n",
       "observed p for \\code{\"bonferroni\"} and \\code{\"holm\"} methods and equal\n",
       "to 1 for the other methods.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "A numeric vector of corrected p-values (of the same length as\n",
       "\\code{p}, with names copied from \\code{p}).\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Benjamini, Y., and Hochberg, Y. (1995).\n",
       "Controlling the false discovery rate: a practical and powerful\n",
       "approach to multiple testing.\n",
       "\\emph{Journal of the Royal Statistical Society Series B} \\bold{57},\n",
       "289--300.\n",
       "\n",
       "Benjamini, Y., and Yekutieli, D. (2001).\n",
       "The control of the false discovery rate in multiple testing under\n",
       "dependency.\n",
       "\\emph{Annals of Statistics} \\bold{29}, 1165--1188.\n",
       "\n",
       "Holm, S. (1979).\n",
       "A simple sequentially rejective multiple test procedure.\n",
       "\\emph{Scandinavian Journal of Statistics} \\bold{6}, 65--70.\n",
       "\n",
       "Hommel, G. (1988).\n",
       "A stagewise rejective multiple test procedure based on a modified\n",
       "Bonferroni test.\n",
       "\\emph{Biometrika} \\bold{75}, 383--386.\n",
       "\n",
       "Hochberg, Y. (1988).\n",
       "A sharper Bonferroni procedure for multiple tests of significance.\n",
       "\\emph{Biometrika} \\bold{75}, 800--803.\n",
       "\n",
       "Shaffer, J. P. (1995).\n",
       "Multiple hypothesis testing.\n",
       "\\emph{Annual Review of Psychology} \\bold{46}, 561--576.\n",
       "(An excellent review of the area.)\n",
       "\n",
       "Sarkar, S. (1998).\n",
       "Some probability inequalities for ordered MTP2 random variables: a\n",
       "proof of Simes conjecture.\n",
       "\\emph{Annals of Statistics} \\bold{26}, 494--504.\n",
       "\n",
       "Sarkar, S., and Chang, C. K. (1997).\n",
       "Simes' method for multiple hypothesis testing with positively\n",
       "dependent test statistics.\n",
       "\\emph{Journal of the American Statistical Association} \\bold{92},\n",
       "1601--1608.\n",
       "\n",
       "Wright, S. P. (1992).\n",
       "Adjusted P-values for simultaneous inference.\n",
       "\\emph{Biometrics} \\bold{48}, 1005--1013.\n",
       "(Explains the adjusted P-value approach.)\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{pairwise.*} functions such as \\code{\\LinkA{pairwise.t.test}{pairwise.t.test}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "require(graphics)\n",
       "\n",
       "set.seed(123)\n",
       "x <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\n",
       "p <- 2*pnorm(sort(-abs(x)))\n",
       "\n",
       "round(p, 3)\n",
       "round(p.adjust(p), 3)\n",
       "round(p.adjust(p, \"BH\"), 3)\n",
       "\n",
       "## or all of them at once (dropping the \"fdr\" alias):\n",
       "p.adjust.M <- p.adjust.methods[p.adjust.methods != \"fdr\"]\n",
       "p.adj    <- sapply(p.adjust.M, function(meth) p.adjust(p, meth))\n",
       "p.adj.60 <- sapply(p.adjust.M, function(meth) p.adjust(p, meth, n = 60))\n",
       "stopifnot(identical(p.adj[,\"none\"], p), p.adj <= p.adj.60)\n",
       "round(p.adj, 3)\n",
       "## or a bit nicer:\n",
       "noquote(apply(p.adj, 2, format.pval, digits = 3))\n",
       "\n",
       "\n",
       "## and a graphic:\n",
       "matplot(p, p.adj, ylab=\"p.adjust(p, meth)\", type = \"l\", asp = 1, lty = 1:6,\n",
       "        main = \"P-value adjustments\")\n",
       "legend(0.7, 0.6, p.adjust.M, col = 1:6, lty = 1:6)\n",
       "\n",
       "## Can work with NA's:\n",
       "pN <- p; iN <- c(46, 47); pN[iN] <- NA\n",
       "pN.a <- sapply(p.adjust.M, function(meth) p.adjust(pN, meth))\n",
       "## The smallest 20 P-values all affected by the NA's :\n",
       "round((pN.a / p.adj)[1:20, ] , 4)\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "p.adjust                 package:stats                 R Documentation\n",
       "\n",
       "_\bA_\bd_\bj_\bu_\bs_\bt _\bP-_\bv_\ba_\bl_\bu_\be_\bs _\bf_\bo_\br _\bM_\bu_\bl_\bt_\bi_\bp_\bl_\be _\bC_\bo_\bm_\bp_\ba_\br_\bi_\bs_\bo_\bn_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Given a set of p-values, returns p-values adjusted using one of\n",
       "     several methods.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     p.adjust(p, method = p.adjust.methods, n = length(p))\n",
       "     \n",
       "     p.adjust.methods\n",
       "     # c(\"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\",\n",
       "     #   \"fdr\", \"none\")\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       p: numeric vector of p-values (possibly with ‘NA’s).  Any other\n",
       "          R is coerced by ‘as.numeric’.\n",
       "\n",
       "  method: correction method.  Can be abbreviated.\n",
       "\n",
       "       n: number of comparisons, must be at least ‘length(p)’; only set\n",
       "          this (to non-default) when you know what you are doing!\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     The adjustment methods include the Bonferroni correction\n",
       "     (‘\"bonferroni\"’) in which the p-values are multiplied by the\n",
       "     number of comparisons.  Less conservative corrections are also\n",
       "     included by Holm (1979) (‘\"holm\"’), Hochberg (1988)\n",
       "     (‘\"hochberg\"’), Hommel (1988) (‘\"hommel\"’), Benjamini & Hochberg\n",
       "     (1995) (‘\"BH\"’ or its alias ‘\"fdr\"’), and Benjamini & Yekutieli\n",
       "     (2001) (‘\"BY\"’), respectively.  A pass-through option (‘\"none\"’)\n",
       "     is also included.  The set of methods are contained in the\n",
       "     ‘p.adjust.methods’ vector for the benefit of methods that need to\n",
       "     have the method as an option and pass it on to ‘p.adjust’.\n",
       "\n",
       "     The first four methods are designed to give strong control of the\n",
       "     family-wise error rate.  There seems no reason to use the\n",
       "     unmodified Bonferroni correction because it is dominated by Holm's\n",
       "     method, which is also valid under arbitrary assumptions.\n",
       "\n",
       "     Hochberg's and Hommel's methods are valid when the hypothesis\n",
       "     tests are independent or when they are non-negatively associated\n",
       "     (Sarkar, 1998; Sarkar and Chang, 1997).  Hommel's method is more\n",
       "     powerful than Hochberg's, but the difference is usually small and\n",
       "     the Hochberg p-values are faster to compute.\n",
       "\n",
       "     The ‘\"BH\"’ (aka ‘\"fdr\"’) and ‘\"BY\"’ method of Benjamini, Hochberg,\n",
       "     and Yekutieli control the false discovery rate, the expected\n",
       "     proportion of false discoveries amongst the rejected hypotheses.\n",
       "     The false discovery rate is a less stringent condition than the\n",
       "     family-wise error rate, so these methods are more powerful than\n",
       "     the others.\n",
       "\n",
       "     Note that you can set ‘n’ larger than ‘length(p)’ which means the\n",
       "     unobserved p-values are assumed to be greater than all the\n",
       "     observed p for ‘\"bonferroni\"’ and ‘\"holm\"’ methods and equal to 1\n",
       "     for the other methods.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     A numeric vector of corrected p-values (of the same length as ‘p’,\n",
       "     with names copied from ‘p’).\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Benjamini, Y., and Hochberg, Y. (1995).  Controlling the false\n",
       "     discovery rate: a practical and powerful approach to multiple\n",
       "     testing.  _Journal of the Royal Statistical Society Series B_\n",
       "     *57*, 289-300.\n",
       "\n",
       "     Benjamini, Y., and Yekutieli, D. (2001).  The control of the false\n",
       "     discovery rate in multiple testing under dependency.  _Annals of\n",
       "     Statistics_ *29*, 1165-1188.\n",
       "\n",
       "     Holm, S. (1979).  A simple sequentially rejective multiple test\n",
       "     procedure.  _Scandinavian Journal of Statistics_ *6*, 65-70.\n",
       "\n",
       "     Hommel, G. (1988).  A stagewise rejective multiple test procedure\n",
       "     based on a modified Bonferroni test.  _Biometrika_ *75*, 383-386.\n",
       "\n",
       "     Hochberg, Y. (1988).  A sharper Bonferroni procedure for multiple\n",
       "     tests of significance.  _Biometrika_ *75*, 800-803.\n",
       "\n",
       "     Shaffer, J. P. (1995).  Multiple hypothesis testing.  _Annual\n",
       "     Review of Psychology_ *46*, 561-576.  (An excellent review of the\n",
       "     area.)\n",
       "\n",
       "     Sarkar, S. (1998).  Some probability inequalities for ordered MTP2\n",
       "     random variables: a proof of Simes conjecture.  _Annals of\n",
       "     Statistics_ *26*, 494-504.\n",
       "\n",
       "     Sarkar, S., and Chang, C. K. (1997).  Simes' method for multiple\n",
       "     hypothesis testing with positively dependent test statistics.\n",
       "     _Journal of the American Statistical Association_ *92*, 1601-1608.\n",
       "\n",
       "     Wright, S. P. (1992).  Adjusted P-values for simultaneous\n",
       "     inference.  _Biometrics_ *48*, 1005-1013.  (Explains the adjusted\n",
       "     P-value approach.)\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘pairwise.*’ functions such as ‘pairwise.t.test’.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     require(graphics)\n",
       "     \n",
       "     set.seed(123)\n",
       "     x <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))\n",
       "     p <- 2*pnorm(sort(-abs(x)))\n",
       "     \n",
       "     round(p, 3)\n",
       "     round(p.adjust(p), 3)\n",
       "     round(p.adjust(p, \"BH\"), 3)\n",
       "     \n",
       "     ## or all of them at once (dropping the \"fdr\" alias):\n",
       "     p.adjust.M <- p.adjust.methods[p.adjust.methods != \"fdr\"]\n",
       "     p.adj    <- sapply(p.adjust.M, function(meth) p.adjust(p, meth))\n",
       "     p.adj.60 <- sapply(p.adjust.M, function(meth) p.adjust(p, meth, n = 60))\n",
       "     stopifnot(identical(p.adj[,\"none\"], p), p.adj <= p.adj.60)\n",
       "     round(p.adj, 3)\n",
       "     ## or a bit nicer:\n",
       "     noquote(apply(p.adj, 2, format.pval, digits = 3))\n",
       "     \n",
       "     \n",
       "     ## and a graphic:\n",
       "     matplot(p, p.adj, ylab=\"p.adjust(p, meth)\", type = \"l\", asp = 1, lty = 1:6,\n",
       "             main = \"P-value adjustments\")\n",
       "     legend(0.7, 0.6, p.adjust.M, col = 1:6, lty = 1:6)\n",
       "     \n",
       "     ## Can work with NA's:\n",
       "     pN <- p; iN <- c(46, 47); pN[iN] <- NA\n",
       "     pN.a <- sapply(p.adjust.M, function(meth) p.adjust(pN, meth))\n",
       "     ## The smallest 20 P-values all affected by the NA's :\n",
       "     round((pN.a / p.adj)[1:20, ] , 4)\n",
       "     "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?p.adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.020604</li>\n",
       "\t<li>0.03039</li>\n",
       "\t<li>0.08552</li>\n",
       "\t<li>0.06936</li>\n",
       "\t<li>0.03045</li>\n",
       "\t<li>1.4574e-05</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.020604\n",
       "\\item 0.03039\n",
       "\\item 0.08552\n",
       "\\item 0.06936\n",
       "\\item 0.03045\n",
       "\\item 1.4574e-05\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.020604\n",
       "2. 0.03039\n",
       "3. 0.08552\n",
       "4. 0.06936\n",
       "5. 0.03045\n",
       "6. 1.4574e-05\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2.0604e-02 3.0390e-02 8.5520e-02 6.9360e-02 3.0450e-02 1.4574e-05"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.adjust(pt,method=\"hommel\",6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.015225</li>\n",
       "\t<li>0.015225</li>\n",
       "\t<li>0.08552</li>\n",
       "\t<li>0.041616</li>\n",
       "\t<li>0.015225</li>\n",
       "\t<li>1.4574e-05</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.015225\n",
       "\\item 0.015225\n",
       "\\item 0.08552\n",
       "\\item 0.041616\n",
       "\\item 0.015225\n",
       "\\item 1.4574e-05\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.015225\n",
       "2. 0.015225\n",
       "3. 0.08552\n",
       "4. 0.041616\n",
       "5. 0.015225\n",
       "6. 1.4574e-05\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1.5225e-02 1.5225e-02 8.5520e-02 4.1616e-02 1.5225e-02 1.4574e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.adjust(pt,method=\"BH\",6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
